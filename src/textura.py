# -*- coding: utf-8 -*-
"""Textura.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HB03mVGDYS-tYePxcr6eWW1NuWKArMR3

Считываем файл
"""

with open(input('Введи название файла '), 'r') as f:
  Text = f.read()

"""Избавляемся от пунктуации. (Внимание, может возникнуть проблема со словами с тире по типу "как-то", "что-то", "кое-где")"""

import nltk
nltk.download("punkt")
words = nltk.word_tokenize(Text)
List_of_Words_without_Punctuation = [word for word in words if word.isalnum()]

"""# Лексическое разнообразие

Установку внешних пакетов, незаданных в гугл колабе, необходимо запускать в отдельной строке. Если мы хотим написать в одном блоке с кодом, то должны использовать пакет os.
"""

!pip install pymorphy2

import pymorphy2
import numpy as np
morph = pymorphy2.MorphAnalyzer()
def Lex_Den(Our_List):
  List_Lemmas = []
  for Word in Our_List:
    List_Lemmas.append(morph.parse(Word)[0].normal_form)
  Words_Quantity = len(List_Lemmas)
  Lemmas_Quantity = len(set(List_Lemmas))
  Lexical_Density = Lemmas_Quantity/Words_Quantity
  print(f'Лексическое разнообразие для данного текста составляет {round(Lexical_Density, 4)}. В тексте встретилось {Words_Quantity} словоформ и {Lemmas_Quantity} слов.')

Lex_Den(List_of_Words_without_Punctuation)

"""## Лексическое разнообразие отдельных равных частей текста.

Разделяем текст на равные части. c_num можем выставлять по нашему желанию. Референс: https://egorovegor.ru/python-chunks/
"""

import math
def func_chunks_num(lst, c_num):
    n = math.ceil(len(lst) / c_num)

    for x in range(0, len(lst), n):
        e_c = lst[x : n + x]

        yield e_c

Splited_List_of_Words_without_Punctuation = list(func_chunks_num(List_of_Words_without_Punctuation, c_num = 10))

for Splited_List in Splited_List_of_Words_without_Punctuation:
  Lex_Den(Splited_List)

"""# Читаемость текста"""

import pandas as pd

"""Диаграмма частотного распределения слов по их длине"""

pd.Series(len(x) for x in ' '.join(List_of_Words_without_Punctuation).split()).value_counts().sort_index().plot(kind='bar', figsize=(12, 3))

Средняя длина слова

AWL = np.mean(np.array([len(va) for va in ' '.join(List_of_Words_without_Punctuation).split()]))

import re
alphabets= "([А-Яа-яЁё])"
def split_into_sentences(text):
    text = " " + text + "  "
    text = text.replace("\n"," ")
    text = re.sub("\s" + alphabets + "[.] "," \\1<prd> ",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>\\3<prd>",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>",text)
    text = re.sub(" " + alphabets + "[.]"," \\1<prd>",text)
    if "”" in text: text = text.replace(".”","”.")
    if "\"" in text: text = text.replace(".\"","\".")
    if "!" in text: text = text.replace("!\"","\"!")
    if "?" in text: text = text.replace("?\"","\"?")
    text = text.replace(".",".<stop>")
    text = text.replace("?","?<stop>")
    text = text.replace("!","!<stop>")
    text = text.replace("<prd>",".")
    sentences = text.split("<stop>")
    sentences = sentences[:-1]
    sentences = [s.strip() for s in sentences]
    return sentences

List_of_Sentences = split_into_sentences(Text)

"""Диаграмма частотного распределения предложений по их длине"""

pd.Series(len(va.split()) for va in List_of_Sentences).value_counts()[:50][::-1].sort_index().plot(kind='bar', figsize=(12, 3))

"""Средняя длинна слов в предложении"""

ASL = np.mean(np.array([len(va.split()) for va in List_of_Sentences]))

pip install pyphen

import pyphen
dic = pyphen.Pyphen(lang='ru_RU')

"""Среднее количество слогов в слове"""

ASW = np.mean(np.array([len(dic.inserted(va).split('-')) for va in ' '.join(List_of_Words_without_Punctuation).split()]))

Reading_Ease_score = 206.835 - (1.015 * ASL) - (84.6 * ASW)

"""Score	Difficulty.  
90-100	Very Easy  
80-89	Easy  
70-79	Fairly Easy  
60-69	Standard  
50-59	Fairly Difficult  
30-49	Difficult  
0-29	Very Confusing
"""

Reading_Ease_score

Hard_Words_Quantity = len(np.array([va for va in ' '.join(List_of_Words_without_Punctuation).split() if len(dic.inserted(va).split('-')) > 2]))

Words_Quantity = len(np.array([va for va in ' '.join(List_of_Words_without_Punctuation).split() if len(dic.inserted(va).split('-'))]))

Gunning_fog = 0.4 * (ASL + ((Hard_Words_Quantity/Words_Quantity)*100) )

Gunning_fog